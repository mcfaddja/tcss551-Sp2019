{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Spark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your full name? (1 point) - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification example (6 points)\n",
    "First, we need to install the sklearn (scikit-learn) package that contains simple and efficient tools for data mining and data analysis. Then, we load a data set from the sklearn.datasets as an example for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as mldata\n",
    "data_dict = mldata.load_breast_cancer() #load the data\n",
    "\n",
    "# translate the data_dict to dataframe\n",
    "cancer = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names']) \n",
    "\n",
    "cancer['bias'] = 1.0 # for the convenience of model fitting\n",
    "\n",
    "# Target data_dict['target'] = 0 is malignant; 1 is benign\n",
    "cancer['malignant'] = 1 - data_dict['target'] \n",
    "cancer.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can conduct one train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#train_test_split in sklearn can help spit the data as follows\n",
    "train, test = train_test_split(cancer, test_size=0.25, random_state=100)\n",
    "x_train = train.drop('malignant', axis=1).values\n",
    "y_train = train['malignant'].values\n",
    "x_test = test.drop('malignant', axis=1).values\n",
    "y_test = test['malignant'].values\n",
    "\n",
    "print(\"Training Data Size: \", len(train))\n",
    "print(\"Test Data Size: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data can be used to fit a LogisticRegression model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(fit_intercept=False, C=1e-5, solver='lbfgs')\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the average train accuracy of the model\n",
    "correct_train = model.predict(x_train) == y_train\n",
    "np.mean(correct_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the average test accuracy of the model\n",
    "correct_test = model.predict(x_test) == y_test\n",
    "np.mean(correct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark example (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing PySpark Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following ones to install Spark locally in the same folder as this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -O http://mirror.metrocast.net/apache/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz \n",
    "#!tar -xvf spark-2.4.2-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python Library will configure your python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to try using Spark on a cluster for free without any setup checkout [Databricks Community Edition](https://databricks.com/try-databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the PySpark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python2\"\n",
    "findspark.init(\"spark-2.4.2-bin-hadoop2.7/\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the SparkSQL session which contains a basic Spark Context.  This may take a few moments to launch the cluster of (typically 4 to 8 python jobs in the background).  Note in a real Spark deployment you would simply change the `.master(\"local[*]\")` to instead point to the YARN resource manager.  To learn more about deploying Spark on a cluster of machines read [this tutorial](https://spark.apache.org/docs/latest/cluster-overview.html).\n",
    "\n",
    "Note: You must have Java installed on your computer for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"LectureExample\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Example\n",
    "\n",
    "As a quick example of what Spark can do, the following code will compute the word counts of pg100 in a parallelized fashion. That means that if your computer has multiple processors, they are all put to use computing the word counts.\n",
    "\n",
    "Below the layer of abstraction that we can see, it is running map reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression used to split lines of text into words\n",
    "\n",
    "lines = sc.textFile(\"./pg100.txt\") # download pg100.txt from canvas in fold of Spark\n",
    "\n",
    "#Split the lines into words (including all alphanumeric characters)\n",
    "words = lines.flatMap(lambda line: re.split(r'[^\\w]+', line))\n",
    "\n",
    "#Mapper\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "#Reducer\n",
    "counts = pairs.reduceByKey(lambda n1, n2: n1 + n2)\n",
    "\n",
    "#Result\n",
    "counts.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to remove some words with specific conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.filter(lambda word: word != '')\n",
    "\n",
    "counts = words.map(lambda word: (word, 1)) \\\n",
    "              .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Additonal comments (e.g., I discussed with XXX, I borrowed from XXX)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "300px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
